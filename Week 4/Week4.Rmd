---
title: "Week4"
author: "Sumit Kant"
date: "14 May 2017"
output: html_document
---

# Regularized Regression

```{r}
library(ElemStatLearn)
data("prostate")
str(prostate)
```

Sometimes you may re-perform the splitting and analysis several times. In order to get a better average estimate of what the out of sample error rate will be. Two common problems
1. Lmimted data
2. Computational Complexity

So another approach is to try to decompose the prediction error. 
So if we assume that the variable y can be predicted as a function of x, plus some error term, then expected prediction error is the expected difference between the outcome and the prediction of the 

Expected mean square error is a composition of
1. Irreducible Erro
2. Bias sqaured
3. Variance

The goal is to reduce the sum of all the above.

## Large predictors small data 

Subsampling will produce estimates for some variables and return NA for others since the data is small and predictors are large
To counter this we can go for hard thresholding.

## Hard Thresholding

# COMBINING PREDICTORS

## Model Stacking
```{r}
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)

Wage <- subset(Wage, select = c(-logwage))

# Building dataset and validation set
inBuild <- createDataPartition(y = Wage$wage, p = 0.7, list = F)
validation <- Wage[-inBuild,]
buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y = buildData$wage, p=0.7, list = F)
training <- buildData[inTrain, ]
testing <- buildData[-inTrain, ]
```

```{r}
dim(training)
```

```{r}
dim(testing)
```

```{r}
dim(validation)
```

## Building two different models

```{r warning = FALSE}
mod1 <- train(wage ~., method = "glm", data = training)
mod2 <- train(wage ~., method = "rf", 
              data = training, 
              trControl = trainControl(method = "cv"), number = 3)
```

## Prediction on testing set

```{r}
pred1 <- predict(mod1, testing)

pred2 <- predict(mod2, testing)

qplot(pred1, pred2, color = wage, data = testing)
```

## Fitting the model that combines the predictors

Build a new data set from predictions of both model 1 and 2
Relation outcome of the predictions from the two models.

```{r message=FALSE}
predDF <- data.frame(pred1, pred2, wage  = testing$wage)
combModFit <- train(wage ~ ., method = "gam", data = predDF)
combPred <- predict(combModFit, predDF)
```

```{r}
qplot(combPred, pred2, color = predDF$wage)
```

```{r}
qplot(combPred, pred1, color = predDF$wage)
```


## Testing Errors

Error in first predictor

```{r}
sqrt(sum((pred1 - testing$wage)^2))
```

Error in second predictor

```{r}
sqrt(sum((pred2 - testing$wage)^2))
```

Error in combine predictor
```{r}
sqrt(sum((combPred - testing$wage)^2))
```

which is lower than both predictor 1 and 2

## Predict on the validation set

```{r warning=FALSE}
pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)
predVDF <- data.frame(pred1 = pred1V, pred2 = pred2V)
combPredV <- predict(combModFit, predVDF)
```

## Evaluation on the validation set

Error in first predictor

```{r}
sqrt(sum((pred1V - validation$wage)^2))
```

Error in second predictor

```{r}
sqrt(sum((pred2V - validation$wage)^2))
```

Error in combine predictor
```{r}
sqrt(sum((combPredV - validation$wage)^2))
```

Stacking models in this way can reduce errors and improve accuracy.


# FORECASTING

## Forecasting Google data

```{r warning=FALSE}
# install.packages("quantmod")
library(quantmod)
from.dat <- as.Date("01/01/10", format = "%m/%d/%y")
to.dat <- as.Date("12/31/14", format = "%m/%d/%y")
getSymbols("MSFT", src ="google", from = from.dat, to = to.dat)
getSymbols("AAPL", src ="google", from = from.dat, to = Sys.Date())
```

```{r}
head(MSFT)
```

## Summarize monthly and store as time series

```{r}
mMsft <- to.monthly(MSFT)
msftOpen <- Op(mMsft)
ts1 <- ts(msftOpen, frequency = 12)
plot(ts1, xlab ="Years + 1", ylab = "MSFT")
```

## Example time series decomposition

* TREND
* SEASONAL
* CYCLIC

```{r}
plot(decompose(ts1), xlab ="Years + 1")
```

## Training and test sets

```{r}
ts1train <- window(ts1, start = 1, end = 5)
ts1test <- window(ts1, start = 5, end = (6 - 0.01))
ts1train
```

## Ways to do forecasting

### SIMPLE MOVING AVERAGE

```{r}
library(forecast)
plot(ts1train)
lines(ma(ts1train, order = 3), col ="red")
```

### EXPONENTIAL SMOOTHING

```{r}
ets1 <- ets(ts1train, model = "MMM")
fcast <- forecast(ets1)
plot(fcast)
lines(ts1test, col ="red")
```

## Getting accuracy

```{r}
accuracy(fcast, ts1test)
```


# UNSUPERVISED PREDICTION 

## Load IRIS data
```{r}
data("iris")
library(ggplot2)

inTrain <- createDataPartition(y = iris$Species, p =0.7, list = F)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training)
```

```{r}
dim(testing)
```


## K-Means clustering
```{r}
kmeans1 <- kmeans(subset(training, select = -c(Species)), centers = 3)
training$clusters <- as.factor(kmeans1$cluster)
qplot(Petal.Width, Petal.Length, color = clusters, data = training)
```

## Compare to real labels

```{r}
table(kmeans1$cluster, training$Species)
```


## Build predictor

```{r}
modFit <- train(clusters ~ ., data = subset(training, select =-c(Species)),
                method = "rpart")
table(predict(modFit, training), training$Species)
```

## Apply on test dataset

```{r}
testCLusterPred <- predict(modFit, testing)
table(testCLusterPred, testing$Species)
```

