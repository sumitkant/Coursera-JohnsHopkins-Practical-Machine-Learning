---
title: "Practical Machine Learning -Project"
author: "Sumit Kant"
date: "28 May 2017"
output:
  md_document:
    variant: markdown_github
---
# Loading dataset

```{r warning = FALSE}
pml <- read.csv("pml-training.csv")
dim(pml)
```
We have dataset with 19622 observations of 160 variables.


## Cleaning dataset

### Missing Values
```{r warning=FALSE}
colmeans <- sapply(pml, function(x){mean(is.na(x))})
missing <- colmeans > 0
pml.1 <- pml[,!missing]
dim(pml.1)
```

After this operation we are left with 93 variables

### Near Zero Variance

Looking out for varibles with near zero variance which would not contribute much in prediciting the class.

```{r warning = FALSE}
library(caret)
nzv <- nearZeroVar(pml.1, saveMetrics = T)
pml.2 <- pml.1[,!nzv$nzv]
pml.2 <- pml.2[,-1]
dim(pml.2)
```

After this operation we found 58 predictors to work with.

## Exploratory Data Analysis

### Considering only one subject - 'carlitos'
```{r warning = FALSE}
# Subsetting for subject 1
carlitos <- subset(pml.2, pml.2$user_name == "carlitos")
dim(carlitos)
```
We have got 3112 observations of 58 variables

### Singular Value decomposition

```{r warning = FALSE}
# Singular Value Decomposition
numeric <- sapply(carlitos, is.numeric)
carlitos.numeric <- carlitos[,numeric]
svd1 = svd(scale(carlitos.numeric))
```

### PLotting Left Singular Vector
```{r warning = FALSE}
# PLotting Left Singular vector
ncol(svd1$u)
par(mfrow=c(5,11), mar = c(2,2,1,1))
for (x in 1:55){
  plot(svd1$u[,x], pch = 20, col = carlitos$classe)
}
```

Plotting the left singular vector we observe that preditors have strong predictive capabilities and there is no single variable that classifies the way in which exercise was performed. So, the next step is find the maximum contributor 

### Finding max contributor
```{r}
# Finding max conributor
ncol(svd1$v)
par(mfrow=c(5,11), mar = c(2,2,1,1))
for (x in 1:55){
  plot(svd1$v[,x], pch = 20, col = carlitos$classe)
}
```

Here, in the right singular vector, the second column has most variance. 

```{r warning = FALSE}
maxContrib <- which.max(svd1$v[,2])
names(carlitos)[maxContrib]
```

Is the max contributor

### Clustering with Maxcontributors
```{r}
distanceMatrix <- dist(carlitos[,c(5:7,maxContrib)])
hclustering <- hclust(distanceMatrix)
source("mypclust.R")
par(mfrow =c(1,1))
myplclust(hclustering, lab.col = unclass(carlitos$classe))
```

These four variables are not able to identify the clusters properly. So trying with K-means clustering.

### K means Clustering

```{r}
# K means clustering
kclust <- kmeans(carlitos.numeric, centers = 5, nstart = 150)
table(kclust$cluster, carlitos$classe)
```

THe clusters formed in k means clustering even after using 150 different starts are too mixed up. There are no variables that show clear demarcation in the classes. So linear regression would not work.

## Predictive Model

* Fitting the model with Gradient Boosting Method Algorithm with 10 fold Cross Validation, since calling gbm with cross validation imporves performance

```{r eval=FALSE}
library(caret)
set.seed(123)
system.time(boostFit <- train(classe ~ ., method = "gbm", 
                  data = pml.2, 
                  verbose = F, 
                  trControl = trainControl(method = "cv", number = 10)))
```

```{r}
boostFit
```

The accuracy of model was 0.996

## Predictions

The final model chosen is the Gradient Boosted model. The final Predictions are predicted as follows.
```{r warning = FALSE}
testing <- read.csv("pml-testing.csv", header  = T)
predictions <- predict(boostFit, newdata = testing)
predictions
```








